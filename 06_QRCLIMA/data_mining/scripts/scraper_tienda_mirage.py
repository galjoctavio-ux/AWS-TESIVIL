import os
import time
import logging
import random
from pathlib import Path
import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from supabase import create_client, Client
from dotenv import load_dotenv

# --- CONFIGURACI√ìN ---
PROVIDER_NAME = "Tienda Mirage"
LOG_FILE = 'scraper_tienda_mirage.log'

# ‚úÖ CATEGOR√çAS QUE PEDISTE
CATEGORIES = [
    {"name": "Aire Acondicionado", "url": "https://www.tiendamirage.mx/11-aire-acondicionado"},
    {"name": "Refacciones", "url": "https://www.tiendamirage.mx/10-refacciones"}
]

# 1. Logging
logging.basicConfig(
    filename=LOG_FILE, 
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# 2. Credenciales
env_path = Path(__file__).parent / ".env"
load_dotenv(dotenv_path=env_path)

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("‚ùå Error: Credenciales no encontradas en .env")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def create_robust_session():
    session = requests.Session()
    retry = Retry(
        total=5, 
        backoff_factor=2, 
        status_forcelist=[429, 500, 502, 503, 504]
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    # Headers mejorados para parecer un humano real y evitar bloqueos
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "es-MX,es;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": "https://www.google.com/"
    })
    return session

def process_category(session, category_info):
    base_url = category_info["url"]
    cat_name = category_info["name"]
    print(f"üìÇ Escaneando: {cat_name}...")
    
    total_items = 0
    page = 1
    
    while True:
        url = f"{base_url}?page={page}"
        print(f"   üìÑ Procesando p√°g {page}: {url}")
        
        try:
            response = session.get(url, timeout=30)
            if response.status_code != 200:
                print(f"      ‚õî Fin o Error {response.status_code}")
                break
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Selectores combinados (PrestaShop est√°ndar + variaciones)
            products = soup.select('.product-miniature') or \
                       soup.select('.js-product-miniature') or \
                       soup.select('article.product-miniature')
            
            if not products:
                # Debug: Si es la p√°gina 1 y no hay productos, algo raro pasa
                if page == 1:
                    print("      ‚ö†Ô∏è Alerta: No se encontraron productos en la p√°gina 1. Posible cambio de dise√±o o bloqueo.")
                else:
                    print("      ‚úÖ Fin de la paginaci√≥n.")
                break
            
            batch_data = []

            for p in products:
                try:
                    # 1. T√≠tulo y Enlace
                    title_tag = p.select_one('.product-title a') or \
                                p.select_one('h3.product-title a') or \
                                p.select_one('h2.product-title a')
                    
                    if not title_tag: continue
                    
                    raw_title = title_tag.get_text(strip=True)
                    link = title_tag['href']
                    
                    # 2. Precio (Limpieza robusta del script antiguo)
                    price_tag = p.select_one('.price') or \
                                p.select_one('[itemprop="price"]') or \
                                p.select_one('.product-price')
                                
                    if not price_tag: continue
                    
                    price_text = price_tag.get_text(strip=True)
                    # Extraer solo n√∫meros y puntos
                    price_clean = "".join(c for c in price_text if c.isdigit() or c == '.')
                    try:
                        price = float(price_clean)
                    except:
                        price = 0.0
                    
                    if price <= 1.0: continue # Ignorar precios 0 o err√≥neos

                    # 3. SKU / ID Interno
                    sku_val = p.get('data-id-product')
                    if not sku_val:
                        # Fallback: intentar sacar el ID del input hidden si existe
                        input_id = p.select_one('input[name="id_product"]')
                        if input_id: sku_val = input_id['value']
                    
                    if not sku_val:
                         # √öltimo recurso: URL slug
                        sku_val = link.split('/')[-1].replace('.html', '')[:20]
                    
                    sku_provider = f"MIR-{sku_val}"

                    # 4. Objeto Nuevo (Esquema log_scraper_prices)
                    item = {
                        "provider_name": PROVIDER_NAME,
                        "sku_provider": sku_provider[:100],
                        "raw_title": raw_title[:500],
                        "price": price,
                        "currency": "MXN",
                        "status": "pending", 
                        "metadata": {
                            "url": link,
                            "category_original": cat_name,
                            "internal_id": sku_val,
                            "scraped_at": time.time()
                        }
                    }
                    batch_data.append(item)

                except Exception as e:
                    continue

            # Insertar lote
            if batch_data:
                try:
                    supabase.table("log_scraper_prices").insert(batch_data).execute()
                    count = len(batch_data)
                    total_items += count
                    print(f"      üíæ Guardados {count} productos.")
                except Exception as e:
                    logging.error(f"Error DB: {e}")
            else:
                print("      ‚ö†Ô∏è Se encontraron contenedores pero no se pudo extraer info v√°lida.")

            # Paginaci√≥n: Buscar bot√≥n "Siguiente"
            next_btn = soup.select_one('a.next') or soup.select_one('a.js-search-link.next')
            
            # Verificar si tiene clase disabled
            if not next_btn or 'disabled' in next_btn.get('class', []):
                print("      ‚úÖ √öltima p√°gina alcanzada.")
                break
            
            page += 1
            # Pausa humana (PrestaShop es sensible a r√°fagas)
            time.sleep(random.uniform(2.0, 4.0))

        except Exception as e:
            logging.error(f"Error cr√≠tico en p√°g {page}: {e}")
            break
            
    return total_items

def run_scraper():
    session = create_robust_session()
    total = 0
    print(f"üöÄ Iniciando Scraper {PROVIDER_NAME}...")
    
    for cat in CATEGORIES:
        total += process_category(session, cat)
        
    print(f"\nüèÅ Finalizado {PROVIDER_NAME}. Total insertados: {total}")

if __name__ == "__main__":
    run_scraper()